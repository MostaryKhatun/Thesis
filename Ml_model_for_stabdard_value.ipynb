{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP44qvdNFKXco7uQOncVzC6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MostaryKhatun/Thesis/blob/main/Ml_model_for_stabdard_value.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TajNPuJklLOT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Upload your dataset"
      ],
      "metadata": {
        "id": "-aVh47lGlaDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "eOXlNpiildYK",
        "outputId": "628f9624-681e-49c6-f4d3-f980e75113e9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-48fd3b62-9fa7-4edb-97b8-a25bd1d74ec9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-48fd3b62-9fa7-4edb-97b8-a25bd1d74ec9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Standard_for_all_dataset.csv to Standard_for_all_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Train the model and save it"
      ],
      "metadata": {
        "id": "5W-ojiVnlrJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle\n",
        "import os\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Loading the dataset\n",
        "dataset_path = '/content/Standard_for_all_dataset.csv'  # Assuming the dataset was uploaded\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(dataset_path):\n",
        "    raise FileNotFoundError(f\"Dataset not found at {dataset_path}\")\n",
        "\n",
        "dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "# Check if the target column exists\n",
        "if 'Production_%' not in dataset.columns:\n",
        "    raise KeyError(\"The column 'Egg_production_percentage' is missing from the dataset.\")\n",
        "\n",
        "# Prepare features (X) and target (y)\n",
        "X = dataset.drop(columns=['Production_%'])\n",
        "y = dataset['Production_%']\n",
        "\n",
        "# Convert categorical columns to numeric codes if necessary\n",
        "categorical_columns = X.select_dtypes(include=['object']).columns\n",
        "for col in categorical_columns:\n",
        "    X[col] = X[col].astype('category').cat.codes\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train different models and evaluate their performance\n",
        "rf_model = RandomForestRegressor()\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "rf_y_pred = rf_model.predict(X_test_scaled)\n",
        "rf_mse = mean_squared_error(y_test, rf_y_pred)\n",
        "rf_r2 = r2_score(y_test, rf_y_pred)\n",
        "\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train_scaled, y_train)\n",
        "lr_y_pred = lr_model.predict(X_test_scaled)\n",
        "lr_mse = mean_squared_error(y_test, lr_y_pred)\n",
        "lr_r2 = r2_score(y_test, lr_y_pred)\n",
        "\n",
        "svr_model = SVR()\n",
        "svr_model.fit(X_train_scaled, y_train)\n",
        "svr_y_pred = svr_model.predict(X_test_scaled)\n",
        "svr_mse = mean_squared_error(y_test, svr_y_pred)\n",
        "svr_r2 = r2_score(y_test, svr_y_pred)\n",
        "\n",
        "# Store models in a dictionary\n",
        "models = {\n",
        "    'RandomForest': {'model': rf_model, 'mse': rf_mse, 'r2': rf_r2},\n",
        "    'LinearRegression': {'model': lr_model, 'mse': lr_mse, 'r2': lr_r2},\n",
        "    'SVR': {'model': svr_model, 'mse': svr_mse, 'r2': svr_r2},\n",
        "}\n",
        "\n",
        "# Select the best model based on Mean Squared Error\n",
        "best_model_name = min(models, key=lambda x: models[x]['mse'])\n",
        "best_model = models[best_model_name]['model']\n",
        "\n",
        "# Save the best model and scaler\n",
        "model_path = '/content/egg_model.sav'\n",
        "scaler_path = '/content/scaler.sav'\n",
        "\n",
        "with open(model_path, 'wb') as model_file:\n",
        "    pickle.dump(best_model, model_file)\n",
        "\n",
        "with open(scaler_path, 'wb') as scaler_file:\n",
        "    pickle.dump(scaler, scaler_file)\n",
        "\n",
        "print(f\"Best model: {best_model_name} with MSE: {models[best_model_name]['mse']} and R2: {models[best_model_name]['r2']}\")\n",
        "print(f\"Models saved as {model_path} and {scaler_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGaXgmszlsME",
        "outputId": "f5cbb09a-b0c3-4860-e76d-9d4c944b2321"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model: RandomForest with MSE: 3.884766666666667 and R2: 0.9974892393498608\n",
            "Models saved as /content/egg_model.sav and /content/scaler.sav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Make predictions with the trained model\n",
        "Once the model and scaler are saved, you can use them for making predictions."
      ],
      "metadata": {
        "id": "nqI4-MItmZJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check column names\n",
        "dataset = pd.read_csv('/content/Standard_for_all_dataset.csv')\n",
        "print(\"Columns in the dataset:\", dataset.columns)\n",
        "\n",
        "# Ensure you are including all necessary input features\n",
        "inputs = [\n",
        "    float(input(\"Enter age in weeks: \")),\n",
        "    float(input(\"Enter feed quantity: \")),\n",
        "    float(input(\"Enter water quantity: \")),\n",
        "    float(input(\"Enter bodyweight: \")),\n",
        "    float(input(\"Enter lighting condition (e.g., hours per day): \")),\n",
        "    float(input(\"Enter any missing feature (if required, e.g., temperature): \"))  # Add the missing feature here\n",
        "]\n",
        "\n",
        "# Scale the inputs\n",
        "scaled_inputs = scaler.transform([inputs])\n",
        "\n",
        "# Make prediction\n",
        "prediction = model.predict(scaled_inputs)[0]\n",
        "print(f\"Predicted Egg Production Percentage: {prediction}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ds9_8XUImaAq",
        "outputId": "f3e47be5-afcb-4ec0-b8f0-ac9fd77d3deb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in the dataset: Index(['Age_week', 'Consumption_feed_g_per_bird_per_day',\n",
            "       'Consumption_water_g_per_bird_per_day', 'Vaccination ', 'Bodyweight_g',\n",
            "       'Lighting_duration_production_hours_per_day', 'Production_%'],\n",
            "      dtype='object')\n",
            "Enter age in weeks: 80\n",
            "Enter feed quantity: 115\n",
            "Enter water quantity: 230\n",
            "Enter bodyweight: 1920\n",
            "Enter lighting condition (e.g., hours per day): 17\n",
            "Enter any missing feature (if required, e.g., temperature): 25\n",
            "Predicted Egg Production Percentage: 65.74\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "again"
      ],
      "metadata": {
        "id": "4vN4h0kyosv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle\n",
        "import os\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(dataset_path, model_path, scaler_path):\n",
        "    try:\n",
        "        # Check if dataset exists\n",
        "        if not os.path.exists(dataset_path):\n",
        "            raise FileNotFoundError(f\"Dataset not found at {dataset_path}\")\n",
        "\n",
        "        # Load the dataset\n",
        "        dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "        # Check if the target column exists\n",
        "        if 'Production_%' not in dataset.columns:\n",
        "            raise KeyError(\"The column 'Production_%' is missing from the dataset.\")\n",
        "\n",
        "        # Feature and target separation\n",
        "        X = dataset.drop(columns=['Production_%'])\n",
        "        y = dataset['Production_%']\n",
        "\n",
        "        # Convert categorical columns to numeric (if any)\n",
        "        categorical_columns = X.select_dtypes(include=['object']).columns\n",
        "        for col in categorical_columns:\n",
        "            X[col] = X[col].astype('category').cat.codes\n",
        "\n",
        "        # Split the data into training and testing sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Standardize the features\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        # Train different models\n",
        "        rf_model = RandomForestRegressor(random_state=42)\n",
        "        rf_model.fit(X_train_scaled, y_train)\n",
        "        rf_y_pred_rf = rf_model.predict(X_test_scaled)\n",
        "        rf_mse = mean_squared_error(y_test, rf_y_pred_rf)\n",
        "        rf_r2 = r2_score(y_test, rf_y_pred_rf)\n",
        "\n",
        "        lr_model = LinearRegression()\n",
        "        lr_model.fit(X_train_scaled, y_train)\n",
        "        lr_y_pred_lr = lr_model.predict(X_test_scaled)\n",
        "        lr_mse = mean_squared_error(y_test, lr_y_pred_lr)\n",
        "        lr_r2 = r2_score(y_test, lr_y_pred_lr)\n",
        "\n",
        "        svr_model = SVR()\n",
        "        svr_model.fit(X_train_scaled, y_train)\n",
        "        svr_y_pred_svr = svr_model.predict(X_test_scaled)\n",
        "        svr_mse = mean_squared_error(y_test, svr_y_pred_svr)\n",
        "        svr_r2 = r2_score(y_test, svr_y_pred_svr)\n",
        "\n",
        "        # Select the best model based on MSE\n",
        "        models = {\n",
        "            'RandomForest': {'model': rf_model, 'mse': rf_mse, 'r2': rf_r2},\n",
        "            'LinearRegression': {'model': lr_model, 'mse': lr_mse, 'r2': lr_r2},\n",
        "            'SVR': {'model': svr_model, 'mse': svr_mse, 'r2': svr_r2},\n",
        "        }\n",
        "\n",
        "        # Get the model with the lowest MSE\n",
        "        best_model_name = min(models, key=lambda x: models[x]['mse'])\n",
        "        best_model = models[best_model_name]['model']\n",
        "\n",
        "        # Save the best model and the scaler\n",
        "        with open(model_path, 'wb') as model_file:\n",
        "            pickle.dump(best_model, model_file)\n",
        "\n",
        "        with open(scaler_path, 'wb') as scaler_file:\n",
        "            pickle.dump(scaler, scaler_file)\n",
        "\n",
        "        print(f\"Model trained and saved successfully! Best model: {best_model_name} with MSE: {models[best_model_name]['mse']}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during training: {e}\")\n",
        "        raise\n",
        "\n",
        "# Function to make a prediction\n",
        "def predict_model(input_data, model_path, scaler_path):\n",
        "    try:\n",
        "        # Check if the model and scaler exist\n",
        "        if not os.path.exists(model_path) or not os.path.exists(scaler_path):\n",
        "            raise FileNotFoundError(\"Model or scaler file not found. Please train the model first.\")\n",
        "\n",
        "        # Load the model and scaler\n",
        "        model = pickle.load(open(model_path, 'rb'))\n",
        "        scaler = pickle.load(open(scaler_path, 'rb'))\n",
        "\n",
        "        # Ensure input data has the correct format\n",
        "        if len(input_data) != 5:\n",
        "            raise ValueError(\"Input data must have exactly 5 features.\")\n",
        "\n",
        "        # Scale the input data\n",
        "        scaled_inputs = scaler.transform([input_data])\n",
        "\n",
        "        # Make the prediction\n",
        "        prediction = model.predict(scaled_inputs)[0]\n",
        "\n",
        "        # Return the prediction\n",
        "        return prediction\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during prediction: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage:\n",
        "dataset_path = '/content/Standard_for_all_dataset.csv'  # Path to your dataset in Colab\n",
        "model_path = '/content/egg_model.sav'  # Path to save the trained model\n",
        "scaler_path = '/content/scaler.sav'  # Path to save the scaler\n",
        "\n",
        "# Training the model\n",
        "train_model(dataset_path, model_path, scaler_path)\n",
        "\n",
        "# Predicting with new inputs\n",
        "inputs = [80, 115, 230, 1920, 17]  # Example input: age, feed, water, bodyweight, lighting\n",
        "prediction = predict_model(inputs, model_path, scaler_path)\n",
        "\n",
        "if prediction is not None:\n",
        "    print(f\"Predicted Egg Production: {prediction}%\")\n",
        "else:\n",
        "    print(\"Prediction failed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X_Hl0fTot9M",
        "outputId": "ce9e772e-3682-4ae7-c7dd-8ba8ec622640"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained and saved successfully! Best model: RandomForest with MSE: 5.0410666666666675\n",
            "Error during prediction: X has 5 features, but StandardScaler is expecting 6 features as input.\n",
            "Prediction failed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle\n",
        "import os\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(dataset_path, model_path, scaler_path):\n",
        "    try:\n",
        "        # Check if dataset exists\n",
        "        if not os.path.exists(dataset_path):\n",
        "            raise FileNotFoundError(f\"Dataset not found at {dataset_path}\")\n",
        "\n",
        "        # Load the dataset\n",
        "        dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "        # Check if the target column exists\n",
        "        if 'Production_%' not in dataset.columns:\n",
        "            raise KeyError(\"The column 'Production_%' is missing from the dataset.\")\n",
        "\n",
        "        # Feature and target separation\n",
        "        X = dataset.drop(columns=['Production_%'])\n",
        "        y = dataset['Production_%']\n",
        "\n",
        "        # Convert categorical columns to numeric (if any)\n",
        "        categorical_columns = X.select_dtypes(include=['object']).columns\n",
        "        for col in categorical_columns:\n",
        "            X[col] = X[col].astype('category').cat.codes\n",
        "\n",
        "        # Check the number of features\n",
        "        print(\"Training features (X) shape:\", X.shape)\n",
        "\n",
        "        # Split the data into training and testing sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Standardize the features\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        # Train different models\n",
        "        rf_model = RandomForestRegressor(random_state=42)\n",
        "        rf_model.fit(X_train_scaled, y_train)\n",
        "        rf_y_pred_rf = rf_model.predict(X_test_scaled)\n",
        "        rf_mse = mean_squared_error(y_test, rf_y_pred_rf)\n",
        "        rf_r2 = r2_score(y_test, rf_y_pred_rf)\n",
        "\n",
        "        lr_model = LinearRegression()\n",
        "        lr_model.fit(X_train_scaled, y_train)\n",
        "        lr_y_pred_lr = lr_model.predict(X_test_scaled)\n",
        "        lr_mse = mean_squared_error(y_test, lr_y_pred_lr)\n",
        "        lr_r2 = r2_score(y_test, lr_y_pred_lr)\n",
        "\n",
        "        svr_model = SVR()\n",
        "        svr_model.fit(X_train_scaled, y_train)\n",
        "        svr_y_pred_svr = svr_model.predict(X_test_scaled)\n",
        "        svr_mse = mean_squared_error(y_test, svr_y_pred_svr)\n",
        "        svr_r2 = r2_score(y_test, svr_y_pred_svr)\n",
        "\n",
        "        # Select the best model based on MSE\n",
        "        models = {\n",
        "            'RandomForest': {'model': rf_model, 'mse': rf_mse, 'r2': rf_r2},\n",
        "            'LinearRegression': {'model': lr_model, 'mse': lr_mse, 'r2': lr_r2},\n",
        "            'SVR': {'model': svr_model, 'mse': svr_mse, 'r2': svr_r2},\n",
        "        }\n",
        "\n",
        "        # Get the model with the lowest MSE\n",
        "        best_model_name = min(models, key=lambda x: models[x]['mse'])\n",
        "        best_model = models[best_model_name]['model']\n",
        "\n",
        "        # Save the best model and the scaler\n",
        "        with open(model_path, 'wb') as model_file:\n",
        "            pickle.dump(best_model, model_file)\n",
        "\n",
        "        with open(scaler_path, 'wb') as scaler_file:\n",
        "            pickle.dump(scaler, scaler_file)\n",
        "\n",
        "        print(f\"Model trained and saved successfully! Best model: {best_model_name} with MSE: {models[best_model_name]['mse']}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during training: {e}\")\n",
        "        raise\n",
        "\n",
        "# Function to make a prediction\n",
        "def predict_model(input_data, model_path, scaler_path):\n",
        "    try:\n",
        "        # Check if the model and scaler exist\n",
        "        if not os.path.exists(model_path) or not os.path.exists(scaler_path):\n",
        "            raise FileNotFoundError(\"Model or scaler file not found. Please train the model first.\")\n",
        "\n",
        "        # Load the model and scaler\n",
        "        model = pickle.load(open(model_path, 'rb'))\n",
        "        scaler = pickle.load(open(scaler_path, 'rb'))\n",
        "\n",
        "        # Ensure input data has the correct format\n",
        "        if len(input_data) != 6:  # Change to 6 if the model expects 6 features\n",
        "            raise ValueError(\"Input data must have exactly 6 features.\")\n",
        "\n",
        "        # Scale the input data\n",
        "        scaled_inputs = scaler.transform([input_data])\n",
        "\n",
        "        # Make the prediction\n",
        "        prediction = model.predict(scaled_inputs)[0]\n",
        "\n",
        "        # Return the prediction\n",
        "        return prediction\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during prediction: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage:\n",
        "dataset_path = '/content/Standard_for_all_dataset.csv'  # Path to your dataset in Colab\n",
        "model_path = '/content/egg_model.sav'  # Path to save the trained model\n",
        "scaler_path = '/content/scaler.sav'  # Path to save the scaler\n",
        "\n",
        "# Training the model (Run this once to train the model)\n",
        "train_model(dataset_path, model_path, scaler_path)\n",
        "\n",
        "# Manual input for prediction\n",
        "age_week = float(input(\"Enter age in weeks: \"))\n",
        "feed = float(input(\"Enter feed quantity: \"))\n",
        "water = float(input(\"Enter water quantity: \"))\n",
        "bodyweight = float(input(\"Enter bodyweight: \"))\n",
        "lighting = float(input(\"Enter lighting condition (hours per day): \"))\n",
        "\n",
        "# Input the data into the model for prediction\n",
        "inputs = [age_week, feed, water, bodyweight, lighting, 0]  # Add a placeholder for the missing feature\n",
        "# Add a missing feature value (e.g., location, season) if applicable.\n",
        "prediction = predict_model(inputs, model_path, scaler_path)\n",
        "\n",
        "if prediction is not None:\n",
        "    print(f\"Predicted Egg Production: {prediction}%\")\n",
        "else:\n",
        "    print(\"Prediction failed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTkyUnS5p1Uj",
        "outputId": "e31df4fb-212d-4c15-989b-27de3705148e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training features (X) shape: (90, 6)\n",
            "Model trained and saved successfully! Best model: RandomForest with MSE: 5.0410666666666675\n",
            "Enter age in weeks: 80\n",
            "Enter feed quantity: 115\n",
            "Enter water quantity: 230\n",
            "Enter bodyweight: 1920\n",
            "Enter lighting condition (hours per day): 17\n",
            "Predicted Egg Production: 59.34%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle\n",
        "import os\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(dataset_path, model_path, scaler_path):\n",
        "    try:\n",
        "        # Check if dataset exists\n",
        "        if not os.path.exists(dataset_path):\n",
        "            raise FileNotFoundError(f\"Dataset not found at {dataset_path}\")\n",
        "\n",
        "        # Load the dataset\n",
        "        dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "        # Check if the target column exists\n",
        "        if 'Production_%' not in dataset.columns:\n",
        "            raise KeyError(\"The column 'Production_%' is missing from the dataset.\")\n",
        "\n",
        "        # Feature and target separation\n",
        "        X = dataset.drop(columns=['Production_%'])\n",
        "        y = dataset['Production_%']\n",
        "\n",
        "        # Convert categorical columns to numeric (if any)\n",
        "        categorical_columns = X.select_dtypes(include=['object']).columns\n",
        "        for col in categorical_columns:\n",
        "            X[col] = X[col].astype('category').cat.codes\n",
        "\n",
        "        # Check the number of features\n",
        "        print(\"Training features (X) shape:\", X.shape)\n",
        "\n",
        "        # Split the data into training and testing sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Standardize the features\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        # Train different models\n",
        "        rf_model = RandomForestRegressor(random_state=42)\n",
        "        rf_model.fit(X_train_scaled, y_train)\n",
        "        rf_y_pred_rf = rf_model.predict(X_test_scaled)\n",
        "        rf_mse = mean_squared_error(y_test, rf_y_pred_rf)\n",
        "        rf_r2 = r2_score(y_test, rf_y_pred_rf)\n",
        "\n",
        "        lr_model = LinearRegression()\n",
        "        lr_model.fit(X_train_scaled, y_train)\n",
        "        lr_y_pred_lr = lr_model.predict(X_test_scaled)\n",
        "        lr_mse = mean_squared_error(y_test, lr_y_pred_lr)\n",
        "        lr_r2 = r2_score(y_test, lr_y_pred_lr)\n",
        "\n",
        "        svr_model = SVR()\n",
        "        svr_model.fit(X_train_scaled, y_train)\n",
        "        svr_y_pred_svr = svr_model.predict(X_test_scaled)\n",
        "        svr_mse = mean_squared_error(y_test, svr_y_pred_svr)\n",
        "        svr_r2 = r2_score(y_test, svr_y_pred_svr)\n",
        "\n",
        "        # Select the best model based on MSE\n",
        "        models = {\n",
        "            'RandomForest': {'model': rf_model, 'mse': rf_mse, 'r2': rf_r2},\n",
        "            'LinearRegression': {'model': lr_model, 'mse': lr_mse, 'r2': lr_r2},\n",
        "            'SVR': {'model': svr_model, 'mse': svr_mse, 'r2': svr_r2},\n",
        "        }\n",
        "\n",
        "        # Get the model with the lowest MSE\n",
        "        best_model_name = min(models, key=lambda x: models[x]['mse'])\n",
        "        best_model = models[best_model_name]['model']\n",
        "\n",
        "        # Save the best model and the scaler\n",
        "        with open(model_path, 'wb') as model_file:\n",
        "            pickle.dump(best_model, model_file)\n",
        "\n",
        "        with open(scaler_path, 'wb') as scaler_file:\n",
        "            pickle.dump(scaler, scaler_file)\n",
        "\n",
        "        print(f\"Model trained and saved successfully! Best model: {best_model_name} with MSE: {models[best_model_name]['mse']}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during training: {e}\")\n",
        "        raise\n",
        "\n",
        "# Function to make a prediction\n",
        "def predict_model(input_data, model_path, scaler_path, actual_value=None):\n",
        "    try:\n",
        "        # Check if the model and scaler exist\n",
        "        if not os.path.exists(model_path) or not os.path.exists(scaler_path):\n",
        "            raise FileNotFoundError(\"Model or scaler file not found. Please train the model first.\")\n",
        "\n",
        "        # Load the model and scaler\n",
        "        model = pickle.load(open(model_path, 'rb'))\n",
        "        scaler = pickle.load(open(scaler_path, 'rb'))\n",
        "\n",
        "        # Ensure input data has the correct format\n",
        "        if len(input_data) != 6:  # Change to 6 if the model expects 6 features\n",
        "            raise ValueError(\"Input data must have exactly 6 features.\")\n",
        "\n",
        "        # Scale the input data\n",
        "        scaled_inputs = scaler.transform([input_data])\n",
        "\n",
        "        # Make the prediction\n",
        "        prediction = model.predict(scaled_inputs)[0]\n",
        "\n",
        "        # Check if the predicted value is greater than the actual value\n",
        "        if actual_value is not None and prediction > actual_value:\n",
        "            print(f\"Prediction is greater than the actual value! Predicted: {prediction}% > Actual: {actual_value}%\")\n",
        "        else:\n",
        "            print(f\"Predicted Egg Production: {prediction}%\")\n",
        "\n",
        "        # Return the prediction\n",
        "        return prediction\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during prediction: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage:\n",
        "dataset_path = '/content/Standard_for_all_dataset.csv'  # Path to your dataset in Colab\n",
        "model_path = '/content/egg_model.sav'  # Path to save the trained model\n",
        "scaler_path = '/content/scaler.sav'  # Path to save the scaler\n",
        "\n",
        "# Training the model (Run this once to train the model)\n",
        "train_model(dataset_path, model_path, scaler_path)\n",
        "\n",
        "# Manual input for prediction\n",
        "age_week = float(input(\"Enter age in weeks: \"))\n",
        "feed = float(input(\"Enter feed quantity: \"))\n",
        "water = float(input(\"Enter water quantity: \"))\n",
        "bodyweight = float(input(\"Enter bodyweight: \"))\n",
        "lighting = float(input(\"Enter lighting condition (hours per day): \"))\n",
        "actual_value = float(input(\"Enter the actual egg production percentage: \"))  # Actual value for comparison\n",
        "\n",
        "# Input the data into the model for prediction\n",
        "inputs = [age_week, feed, water, bodyweight, lighting, 0]  # Add a placeholder for the missing feature\n",
        "# Add a missing feature value (e.g., location, season) if applicable.\n",
        "prediction = predict_model(inputs, model_path, scaler_path, actual_value)\n",
        "\n",
        "if prediction is not None:\n",
        "    print(f\"Predicted Egg Production: {prediction}%\")\n",
        "else:\n",
        "    print(\"Prediction failed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXdMikE9qo_F",
        "outputId": "64672262-4c5d-47f8-8372-d1a7c8a431f0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training features (X) shape: (90, 6)\n",
            "Model trained and saved successfully! Best model: RandomForest with MSE: 5.0410666666666675\n",
            "Enter age in weeks: 80\n",
            "Enter feed quantity: 115\n",
            "Enter water quantity: 230\n",
            "Enter bodyweight: 1920\n",
            "Enter lighting condition (hours per day): 17\n",
            "Enter the actual egg production percentage: 72\n",
            "Predicted Egg Production: 59.34%\n",
            "Predicted Egg Production: 59.34%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(dataset_path, model_path, scaler_path):\n",
        "    try:\n",
        "        if not os.path.exists(dataset_path):\n",
        "            raise FileNotFoundError(f\"Dataset not found at {dataset_path}\")\n",
        "\n",
        "        # Load the dataset\n",
        "        dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "        if 'Production_%' not in dataset.columns:\n",
        "            raise KeyError(\"The column 'Production_%' is missing from the dataset.\")\n",
        "\n",
        "        # Feature and target separation\n",
        "        X = dataset.drop(columns=['Production_%'])\n",
        "        y = dataset['Production_%']\n",
        "\n",
        "        # Convert categorical columns to numeric (if any)\n",
        "        categorical_columns = X.select_dtypes(include=['object']).columns\n",
        "        for col in categorical_columns:\n",
        "            X[col] = X[col].astype('category').cat.codes\n",
        "\n",
        "        # Save feature names for consistent input handling\n",
        "        feature_names = X.columns.tolist()\n",
        "\n",
        "        # Split the data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Standardize the features\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        # Train Random Forest model\n",
        "        rf_model = RandomForestRegressor(random_state=42)\n",
        "        rf_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "        # Save the model, scaler, and feature names\n",
        "        with open(model_path, 'wb') as model_file:\n",
        "            pickle.dump(rf_model, model_file)\n",
        "        with open(scaler_path, 'wb') as scaler_file:\n",
        "            pickle.dump(scaler, scaler_file)\n",
        "\n",
        "        feature_path = os.path.splitext(model_path)[0] + '_features.pkl'\n",
        "        with open(feature_path, 'wb') as feature_file:\n",
        "            pickle.dump(feature_names, feature_file)\n",
        "\n",
        "        print(\"Model trained and saved successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during training: {e}\")\n",
        "        raise\n",
        "\n",
        "# Function to make a prediction\n",
        "def predict_model(model_path, scaler_path):\n",
        "    try:\n",
        "        # Input values manually\n",
        "        input_data = {\n",
        "            'Age_week': float(input(\"Enter age in weeks: \")),\n",
        "            'Consumption_feed_g_per_bird_per_day': float(input(\"Enter feed quantity: \")),\n",
        "            'Consumption_water_g_per_bird_per_day': float(input(\"Enter water quantity: \")),\n",
        "            'Bodyweight_g': float(input(\"Enter bodyweight: \")),\n",
        "            'Lighting_duration_production_hours_per_day': float(input(\"Enter lighting condition (hours per day): \")),\n",
        "        }\n",
        "\n",
        "        # Load model, scaler, and feature names\n",
        "        model = pickle.load(open(model_path, 'rb'))\n",
        "        scaler = pickle.load(open(scaler_path, 'rb'))\n",
        "\n",
        "        feature_path = os.path.splitext(model_path)[0] + '_features.pkl'\n",
        "        feature_names = pickle.load(open(feature_path, 'rb'))\n",
        "\n",
        "        # Ensure all features are present in the input\n",
        "        for feature in feature_names:\n",
        "            if feature not in input_data:\n",
        "                input_data[feature] = 0  # Default value for missing features (e.g., 'Vaccination')\n",
        "\n",
        "        # Ensure the input data matches the training feature order\n",
        "        input_df = pd.DataFrame([input_data])[feature_names]\n",
        "\n",
        "        # Scale the input data\n",
        "        scaled_inputs = scaler.transform(input_df)\n",
        "\n",
        "        # Make the prediction\n",
        "        prediction = model.predict(scaled_inputs)[0]\n",
        "\n",
        "        print(f\"Predicted Egg Production: {prediction}%\")\n",
        "        return prediction\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during prediction: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage\n",
        "dataset_path = '/content/Standard_for_all_dataset.csv'\n",
        "model_path = '/content/egg_model.sav'\n",
        "scaler_path = '/content/scaler.sav'\n",
        "\n",
        "# Train the model\n",
        "train_model(dataset_path, model_path, scaler_path)\n",
        "\n",
        "# Predict manually\n",
        "predict_model(model_path, scaler_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Bil8cp9rK9-",
        "outputId": "4fb17ab4-4511-4f37-f196-606e7ff4f87c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained and saved successfully!\n",
            "Enter age in weeks: 75\n",
            "Enter feed quantity: 115\n",
            "Enter water quantity: 230\n",
            "Enter bodyweight: 1910\n",
            "Enter lighting condition (hours per day): 17\n",
            "Predicted Egg Production: 76.26%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "76.26"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}